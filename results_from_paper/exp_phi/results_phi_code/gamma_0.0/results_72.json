[
    {
        "epoch": 1,
        "loss": 1.0885154008865356,
        "best_loss": 1.0885154008865356,
        "best_kl": 17.69951057434082,
        "best_std": 0.7164658546447754,
        "cur_kl": 17.69951057434082,
        "cur_std": 0.7164658546447754,
        "prompt": "Prompt: Writeieft Python function called `count_vowels`",
        "nll_prompt": -4.625000953674316
    },
    {
        "epoch": 2,
        "loss": 0.9749481678009033,
        "best_loss": 0.9749481678009033,
        "best_kl": 14.707382202148438,
        "best_std": 0.6291306972503662,
        "cur_kl": 14.707382202148438,
        "cur_std": 0.6291306972503662,
        "prompt": "Prompt: Writeieft Python functiongently `count_vowels`",
        "nll_prompt": -6.251982688903809
    },
    {
        "epoch": 3,
        "loss": 0.86201411485672,
        "best_loss": 0.86201411485672,
        "best_kl": 12.489362716674805,
        "best_std": 0.6189157485961914,
        "cur_kl": 12.489362716674805,
        "cur_std": 0.6189157485961914,
        "prompt": "Prompt: fosterieft Python functiongently `count_vowels`",
        "nll_prompt": -6.289981365203857
    },
    {
        "epoch": 4,
        "loss": 0.8679670691490173,
        "best_loss": 0.86201411485672,
        "best_kl": 12.489362716674805,
        "best_std": 0.6189157485961914,
        "cur_kl": 15.072074890136719,
        "cur_std": 0.7043857097625732,
        "prompt": "PromptERN fosterieft Python functiongently `count_vowels`",
        "nll_prompt": -7.781968116760254
    },
    {
        "epoch": 5,
        "loss": 0.9757716655731201,
        "best_loss": 0.86201411485672,
        "best_kl": 12.489362716674805,
        "best_std": 0.6189157485961914,
        "cur_kl": 13.977922439575195,
        "cur_std": 0.6302712917327881,
        "prompt": "PromptERN fosterieft Python Rogersgently `count_vowels`",
        "nll_prompt": -8.793804168701172
    },
    {
        "epoch": 6,
        "loss": 0.8450197577476501,
        "best_loss": 0.8450197577476501,
        "best_kl": 12.489362716674805,
        "best_std": 0.6189157485961914,
        "cur_kl": 13.121519088745117,
        "cur_std": 0.6400288581848145,
        "prompt": "PromptERN�ieft Python Rogersgently `count_vowels`",
        "nll_prompt": -9.232471466064453
    },
    {
        "epoch": 7,
        "loss": 0.8839332461357117,
        "best_loss": 0.8450197577476501,
        "best_kl": 12.489362716674805,
        "best_std": 0.6189157485961914,
        "cur_kl": 13.525801658630371,
        "cur_std": 0.6580852031707763,
        "prompt": "itchptERN�ieft Python Rogersgently `count_vowels`",
        "nll_prompt": -9.10308837890625
    },
    {
        "epoch": 8,
        "loss": 0.8376539349555969,
        "best_loss": 0.8376539349555969,
        "best_kl": 12.325439453125,
        "best_std": 0.6222629070281982,
        "cur_kl": 12.325439453125,
        "cur_std": 0.6222629070281982,
        "prompt": "itchptERN Rhieft Python Rogersgently `count_vowels`",
        "nll_prompt": -8.843706130981445
    },
    {
        "epoch": 9,
        "loss": 0.8736041188240051,
        "best_loss": 0.8376539349555969,
        "best_kl": 11.284286499023438,
        "best_std": 0.6429751873016357,
        "cur_kl": 11.284286499023438,
        "cur_std": 0.6429751873016357,
        "prompt": "itchptERN Rhieft Python Socialgently `count_vowels`",
        "nll_prompt": -8.6710786819458
    },
    {
        "epoch": 10,
        "loss": 0.85463547706604,
        "best_loss": 0.8376539349555969,
        "best_kl": 11.284286499023438,
        "best_std": 0.6429751873016357,
        "cur_kl": 11.945213317871094,
        "cur_std": 0.6291839122772217,
        "prompt": "itchiERN Rhieft Python Socialgently `count_vowels`",
        "nll_prompt": -8.506037712097168
    },
    {
        "epoch": 11,
        "loss": 0.850967526435852,
        "best_loss": 0.8376539349555969,
        "best_kl": 11.284286499023438,
        "best_std": 0.6429751873016357,
        "cur_kl": 12.170275688171387,
        "cur_std": 0.5974483013153076,
        "prompt": "itchiERN Rhieft Python Passengergently `count_vowels`",
        "nll_prompt": -8.578404426574707
    },
    {
        "epoch": 12,
        "loss": 0.8932059407234192,
        "best_loss": 0.8376539349555969,
        "best_kl": 11.284286499023438,
        "best_std": 0.6429751873016357,
        "cur_kl": 12.415180206298828,
        "cur_std": 0.5821553707122803,
        "prompt": "itchiERN Rh off Python Passengergently `count_vowels`",
        "nll_prompt": -8.505792617797852
    },
    {
        "epoch": 13,
        "loss": 0.7980453372001648,
        "best_loss": 0.7980453372001648,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 10.736991882324219,
        "cur_std": 0.5756730556488037,
        "prompt": " careiERN Rh off Python Passengergently `count_vowels`",
        "nll_prompt": -8.296141624450684
    },
    {
        "epoch": 14,
        "loss": 0.8288700580596924,
        "best_loss": 0.7980453372001648,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 11.291304588317871,
        "cur_std": 0.5872556686401367,
        "prompt": " carei conservation Rh off Python Passengergently `count_vowels`",
        "nll_prompt": -8.588644981384277
    },
    {
        "epoch": 15,
        "loss": 0.9505936503410339,
        "best_loss": 0.7980453372001648,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 11.869367599487305,
        "cur_std": 0.6144907951354981,
        "prompt": " careah conservation Rh off Python Passengergently `count_vowels`",
        "nll_prompt": -8.920666694641113
    },
    {
        "epoch": 16,
        "loss": 0.9230534434318542,
        "best_loss": 0.7980453372001648,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 13.2925386428833,
        "cur_std": 0.6343318462371826,
        "prompt": "conditionah conservation Rh off Python Passengergently `count_vowels`",
        "nll_prompt": -9.71600341796875
    },
    {
        "epoch": 17,
        "loss": 0.8939206004142761,
        "best_loss": 0.7980453372001648,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 11.764691352844238,
        "cur_std": 0.605653190612793,
        "prompt": "conditionah conservation Rh off Python Vegangently `count_vowels`",
        "nll_prompt": -9.45841121673584
    },
    {
        "epoch": 18,
        "loss": 0.8425282835960388,
        "best_loss": 0.7980453372001648,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 12.688337326049805,
        "cur_std": 0.6275678157806397,
        "prompt": "conditionah conservation Rh offed Vegangently `count_vowels`",
        "nll_prompt": -9.449505805969238
    },
    {
        "epoch": 19,
        "loss": 0.803960382938385,
        "best_loss": 0.7980453372001648,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 11.334033966064453,
        "cur_std": 0.6132462024688721,
        "prompt": "ceah conservation Rh offed Vegangently `count_vowels`",
        "nll_prompt": -8.82431697845459
    },
    {
        "epoch": 20,
        "loss": 0.8526696562767029,
        "best_loss": 0.7980453372001648,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 11.194080352783203,
        "cur_std": 0.6029407501220703,
        "prompt": "ceah conservation Rh Elizabethed Vegangently `count_vowels`",
        "nll_prompt": -8.795464515686035
    },
    {
        "epoch": 21,
        "loss": 0.8510502576828003,
        "best_loss": 0.7980453372001648,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 11.017657279968262,
        "cur_std": 0.5928213119506835,
        "prompt": "ceah conservation Rh Sworded Vegangently `count_vowels`",
        "nll_prompt": -8.690107345581055
    },
    {
        "epoch": 22,
        "loss": 0.8514029383659363,
        "best_loss": 0.7980453372001648,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 11.017317771911621,
        "cur_std": 0.571485185623169,
        "prompt": "ceah conservation Rh Rodgersed Vegangently `count_vowels`",
        "nll_prompt": -8.876957893371582
    },
    {
        "epoch": 23,
        "loss": 0.9042824506759644,
        "best_loss": 0.7980453372001648,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 12.302207946777344,
        "cur_std": 0.590960168838501,
        "prompt": "ceah conservation Rh Rodgersed Veganify `count_vowels`",
        "nll_prompt": -8.37179946899414
    },
    {
        "epoch": 24,
        "loss": 0.8925362825393677,
        "best_loss": 0.7980453372001648,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 11.791790962219238,
        "cur_std": 0.6358132839202881,
        "prompt": "ceah conservation RhPINed Veganify `count_vowels`",
        "nll_prompt": -8.210198402404785
    },
    {
        "epoch": 25,
        "loss": 0.8448508977890015,
        "best_loss": 0.7980453372001648,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 12.82780647277832,
        "cur_std": 0.627509593963623,
        "prompt": " newah conservation RhPINed Veganify `count_vowels`",
        "nll_prompt": -8.316521644592285
    },
    {
        "epoch": 26,
        "loss": 0.8692963123321533,
        "best_loss": 0.7980453372001648,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 13.500641822814941,
        "cur_std": 0.6180548667907715,
        "prompt": " newah conservation Rh defed Veganify `count_vowels`",
        "nll_prompt": -8.211193084716797
    },
    {
        "epoch": 27,
        "loss": 0.879744827747345,
        "best_loss": 0.7980453372001648,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 13.533411979675293,
        "cur_std": 0.5804531574249268,
        "prompt": " newah taxes Rh defed Veganify `count_vowels`",
        "nll_prompt": -7.828154563903809
    },
    {
        "epoch": 28,
        "loss": 0.9333717226982117,
        "best_loss": 0.7980453372001648,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 11.6412353515625,
        "cur_std": 0.5702524185180664,
        "prompt": " vocah taxes Rh defed Veganify `count_vowels`",
        "nll_prompt": -7.4631829261779785
    },
    {
        "epoch": 29,
        "loss": 0.7676904797554016,
        "best_loss": 0.7676904797554016,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 12.226598739624023,
        "cur_std": 0.6216163635253906,
        "prompt": " voc delays taxes Rh defed Veganify `count_vowels`",
        "nll_prompt": -7.92627477645874
    },
    {
        "epoch": 30,
        "loss": 0.8256956934928894,
        "best_loss": 0.7676904797554016,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 11.080575942993164,
        "cur_std": 0.582590913772583,
        "prompt": " voc delays diabetes Rh defed Veganify `count_vowels`",
        "nll_prompt": -7.896681785583496
    },
    {
        "epoch": 31,
        "loss": 0.8939036130905151,
        "best_loss": 0.7676904797554016,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 12.347290992736816,
        "cur_std": 0.5880786418914795,
        "prompt": " voc delays diabetes Rh Premed Veganify `count_vowels`",
        "nll_prompt": -9.18192195892334
    },
    {
        "epoch": 32,
        "loss": 0.8692979216575623,
        "best_loss": 0.7676904797554016,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 12.356974601745605,
        "cur_std": 0.5970425605773926,
        "prompt": " voc delays diabetes Rh guardinged Veganify `count_vowels`",
        "nll_prompt": -8.75367546081543
    },
    {
        "epoch": 33,
        "loss": 0.8248191475868225,
        "best_loss": 0.7676904797554016,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 12.547380447387695,
        "cur_std": 0.5912048816680908,
        "prompt": " voc delays diabetes Rh guarding PubMed Veganify `count_vowels`",
        "nll_prompt": -8.86124324798584
    },
    {
        "epoch": 34,
        "loss": 0.8746183514595032,
        "best_loss": 0.7676904797554016,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 11.923410415649414,
        "cur_std": 0.5876340866088867,
        "prompt": " voc delays diabetes Rh Belg PubMed Veganify `count_vowels`",
        "nll_prompt": -8.601113319396973
    },
    {
        "epoch": 35,
        "loss": 0.8713802695274353,
        "best_loss": 0.7676904797554016,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 12.109516143798828,
        "cur_std": 0.6035770416259766,
        "prompt": " voc outcomes diabetes Rh Belg PubMed Veganify `count_vowels`",
        "nll_prompt": -8.562573432922363
    },
    {
        "epoch": 36,
        "loss": 0.8759565353393555,
        "best_loss": 0.7676904797554016,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 12.764933586120605,
        "cur_std": 0.607490062713623,
        "prompt": " voc outcomes Sant Rh Belg PubMed Veganify `count_vowels`",
        "nll_prompt": -9.234743118286133
    },
    {
        "epoch": 37,
        "loss": 0.8609738945960999,
        "best_loss": 0.7676904797554016,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 12.926109313964844,
        "cur_std": 0.5949601650238037,
        "prompt": " voc outcomes review Rh Belg PubMed Veganify `count_vowels`",
        "nll_prompt": -8.797441482543945
    },
    {
        "epoch": 38,
        "loss": 0.9202203750610352,
        "best_loss": 0.7676904797554016,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 12.434858322143555,
        "cur_std": 0.5809527397155761,
        "prompt": " voc outcomes reviewry Belg PubMed Veganify `count_vowels`",
        "nll_prompt": -8.773816108703613
    },
    {
        "epoch": 39,
        "loss": 0.942087709903717,
        "best_loss": 0.7676904797554016,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 12.055757522583008,
        "cur_std": 0.6043097972869873,
        "prompt": " voc outcomes review Cats Belg PubMed Veganify `count_vowels`",
        "nll_prompt": -9.03499698638916
    },
    {
        "epoch": 40,
        "loss": 0.8669847846031189,
        "best_loss": 0.7676904797554016,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 13.027008056640625,
        "cur_std": 0.630625057220459,
        "prompt": " voc outcomes review Cats Belg personal Veganify `count_vowels`",
        "nll_prompt": -8.88479995727539
    },
    {
        "epoch": 41,
        "loss": 0.9349725246429443,
        "best_loss": 0.7676904797554016,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 13.53040599822998,
        "cur_std": 0.6881906509399414,
        "prompt": " voc outcomes review Cats Belg personal Vegan persuasive `count_vowels`",
        "nll_prompt": -8.973329544067383
    },
    {
        "epoch": 42,
        "loss": 0.8740769028663635,
        "best_loss": 0.7676904797554016,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 13.339038848876953,
        "cur_std": 0.660743522644043,
        "prompt": " voc outcomes review Cats Belg personal Vegan Area `count_vowels`",
        "nll_prompt": -9.28686237335205
    },
    {
        "epoch": 43,
        "loss": 0.9374344944953918,
        "best_loss": 0.7676904797554016,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 13.883481979370117,
        "cur_std": 0.6681778430938721,
        "prompt": " voc outcomes review Cats Mau personal Vegan Area `count_vowels`",
        "nll_prompt": -9.144096374511719
    },
    {
        "epoch": 44,
        "loss": 0.8582550883293152,
        "best_loss": 0.7676904797554016,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 13.551410675048828,
        "cur_std": 0.675091028213501,
        "prompt": " voc unfortunate review Cats Mau personal Vegan Area `count_vowels`",
        "nll_prompt": -9.037073135375977
    },
    {
        "epoch": 45,
        "loss": 0.9125899076461792,
        "best_loss": 0.7676904797554016,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 13.798948287963867,
        "cur_std": 0.6936252117156982,
        "prompt": " voc unfortunate review Cats Mau bodily Vegan Area `count_vowels`",
        "nll_prompt": -9.240015029907227
    },
    {
        "epoch": 46,
        "loss": 0.891629695892334,
        "best_loss": 0.7676904797554016,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 13.51076602935791,
        "cur_std": 0.6700905799865723,
        "prompt": " voc unfortunate review London Mau bodily Vegan Area `count_vowels`",
        "nll_prompt": -9.20724868774414
    },
    {
        "epoch": 47,
        "loss": 0.8233830332756042,
        "best_loss": 0.7676904797554016,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 14.046269416809082,
        "cur_std": 0.7121848106384278,
        "prompt": " voc unfortunate oil London Mau bodily Vegan Area `count_vowels`",
        "nll_prompt": -9.184260368347168
    },
    {
        "epoch": 48,
        "loss": 0.83456951379776,
        "best_loss": 0.7676904797554016,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 12.90017032623291,
        "cur_std": 0.6808329105377198,
        "prompt": " voc unfortunate oil London Mau Galaxy Vegan Area `count_vowels`",
        "nll_prompt": -8.957109451293945
    },
    {
        "epoch": 49,
        "loss": 0.8385353088378906,
        "best_loss": 0.7676904797554016,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 12.943108558654785,
        "cur_std": 0.6670008182525635,
        "prompt": " voc unfortunate oil Do Mau Galaxy Vegan Area `count_vowels`",
        "nll_prompt": -8.57717227935791
    },
    {
        "epoch": 50,
        "loss": 0.8308918476104736,
        "best_loss": 0.7676904797554016,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 11.844680786132812,
        "cur_std": 0.6524484157562256,
        "prompt": " voc unfortunate fun Do Mau Galaxy Vegan Area `count_vowels`",
        "nll_prompt": -8.171795845031738
    },
    {
        "epoch": 51,
        "loss": 0.7955499887466431,
        "best_loss": 0.7676904797554016,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 12.961405754089355,
        "cur_std": 0.6592123985290528,
        "prompt": " voc unfortunate fun Amb Mau Galaxy Vegan Area `count_vowels`",
        "nll_prompt": -8.96854019165039
    },
    {
        "epoch": 52,
        "loss": 0.8768142461776733,
        "best_loss": 0.7676904797554016,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 11.886279106140137,
        "cur_std": 0.6367642402648925,
        "prompt": " voc unfortunate fun AmbUT Galaxy Vegan Area `count_vowels`",
        "nll_prompt": -9.213056564331055
    },
    {
        "epoch": 53,
        "loss": 0.7896460294723511,
        "best_loss": 0.7676904797554016,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 11.859106063842773,
        "cur_std": 0.6723752498626709,
        "prompt": " voc unfortunate fun assignUT Galaxy Vegan Area `count_vowels`",
        "nll_prompt": -9.163294792175293
    },
    {
        "epoch": 54,
        "loss": 0.7615708112716675,
        "best_loss": 0.7615708112716675,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 12.236044883728027,
        "cur_std": 0.6949669361114502,
        "prompt": " voc unfortunate fun assignUT Galaxy Luckily Area `count_vowels`",
        "nll_prompt": -9.153509140014648
    },
    {
        "epoch": 55,
        "loss": 0.9249443411827087,
        "best_loss": 0.7615708112716675,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 12.483854293823242,
        "cur_std": 0.710610818862915,
        "prompt": " voc unfortunate fun assign return Galaxy Luckily Area `count_vowels`",
        "nll_prompt": -7.946446895599365
    },
    {
        "epoch": 56,
        "loss": 0.8493056893348694,
        "best_loss": 0.7615708112716675,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 13.08156681060791,
        "cur_std": 0.7323049545288086,
        "prompt": " voc unfortunate fun assignratch Galaxy Luckily Area `count_vowels`",
        "nll_prompt": -8.755849838256836
    },
    {
        "epoch": 57,
        "loss": 0.8667894005775452,
        "best_loss": 0.7615708112716675,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 12.523072242736816,
        "cur_std": 0.7464872360229492,
        "prompt": " voc unfortunate fun assign reception Galaxy Luckily Area `count_vowels`",
        "nll_prompt": -8.609272003173828
    },
    {
        "epoch": 58,
        "loss": 0.9201687574386597,
        "best_loss": 0.7615708112716675,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 12.751209259033203,
        "cur_std": 0.7285247325897217,
        "prompt": " voc unfortunate fun assign reception Pop Luckily Area `count_vowels`",
        "nll_prompt": -8.526177406311035
    },
    {
        "epoch": 59,
        "loss": 0.9270325899124146,
        "best_loss": 0.7615708112716675,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 13.206920623779297,
        "cur_std": 0.723407793045044,
        "prompt": " voc unfortunate attractive assign reception Pop Luckily Area `count_vowels`",
        "nll_prompt": -8.835886001586914
    },
    {
        "epoch": 60,
        "loss": 0.9436432719230652,
        "best_loss": 0.7615708112716675,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 12.366918563842773,
        "cur_std": 0.6570167541503906,
        "prompt": " voc unfortunate attractive assign reception Pop result Area `count_vowels`",
        "nll_prompt": -8.705361366271973
    },
    {
        "epoch": 61,
        "loss": 0.821772038936615,
        "best_loss": 0.7615708112716675,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 12.386829376220703,
        "cur_std": 0.6354072093963623,
        "prompt": " voc unfortunate attractive assign reception same result Area `count_vowels`",
        "nll_prompt": -8.087377548217773
    },
    {
        "epoch": 62,
        "loss": 0.8885546922683716,
        "best_loss": 0.7615708112716675,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 11.623488426208496,
        "cur_std": 0.6493634700775146,
        "prompt": " voc unfortunate flare assign reception same result Area `count_vowels`",
        "nll_prompt": -8.407350540161133
    },
    {
        "epoch": 63,
        "loss": 0.7715640664100647,
        "best_loss": 0.7615708112716675,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 11.590723991394043,
        "cur_std": 0.626125717163086,
        "prompt": " voc unfortunate flare assign reception ut result Area `count_vowels`",
        "nll_prompt": -8.705493927001953
    },
    {
        "epoch": 64,
        "loss": 0.8704107999801636,
        "best_loss": 0.7615708112716675,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 11.04654598236084,
        "cur_std": 0.6597050189971924,
        "prompt": " voc unfortunate flare assign reception% result Area `count_vowels`",
        "nll_prompt": -9.08882999420166
    },
    {
        "epoch": 65,
        "loss": 0.9029595255851746,
        "best_loss": 0.7615708112716675,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 12.0604248046875,
        "cur_std": 0.6459456443786621,
        "prompt": "urry unfortunate flare assign reception% result Area `count_vowels`",
        "nll_prompt": -8.432219505310059
    },
    {
        "epoch": 66,
        "loss": 0.9148547053337097,
        "best_loss": 0.7615708112716675,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 13.738117218017578,
        "cur_std": 0.6649991512298584,
        "prompt": "urry unfortunate flare assign reception% learning Area `count_vowels`",
        "nll_prompt": -8.762359619140625
    },
    {
        "epoch": 67,
        "loss": 0.9203762412071228,
        "best_loss": 0.7615708112716675,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 14.772801399230957,
        "cur_std": 0.6273651123046875,
        "prompt": " Am unfortunate flare assign reception% learning Area `count_vowels`",
        "nll_prompt": -8.771201133728027
    },
    {
        "epoch": 68,
        "loss": 0.8978574275970459,
        "best_loss": 0.7615708112716675,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 14.07778263092041,
        "cur_std": 0.6629536151885986,
        "prompt": " un unfortunate flare assign reception% learning Area `count_vowels`",
        "nll_prompt": -8.785511016845703
    },
    {
        "epoch": 69,
        "loss": 0.9225417375564575,
        "best_loss": 0.7615708112716675,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 13.99679946899414,
        "cur_std": 0.6652191638946533,
        "prompt": " un unfortunate flareation reception% learning Area `count_vowels`",
        "nll_prompt": -8.643242835998535
    },
    {
        "epoch": 70,
        "loss": 0.8418582081794739,
        "best_loss": 0.7615708112716675,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 13.165851593017578,
        "cur_std": 0.6352582931518554,
        "prompt": " un unfortunate meetsation reception% learning Area `count_vowels`",
        "nll_prompt": -8.393423080444336
    },
    {
        "epoch": 71,
        "loss": 0.8595415353775024,
        "best_loss": 0.7615708112716675,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 13.08617115020752,
        "cur_std": 0.6285187721252441,
        "prompt": " un unfortunate meetsation impression% learning Area `count_vowels`",
        "nll_prompt": -8.288125038146973
    },
    {
        "epoch": 72,
        "loss": 0.9502450227737427,
        "best_loss": 0.7615708112716675,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 13.875052452087402,
        "cur_std": 0.6356357097625732,
        "prompt": " un unfortunate meets Alice impression% learning Area `count_vowels`",
        "nll_prompt": -8.230475425720215
    },
    {
        "epoch": 73,
        "loss": 0.8809554576873779,
        "best_loss": 0.7615708112716675,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 13.7193603515625,
        "cur_std": 0.6277379989624023,
        "prompt": " un Welcome meets Alice impression% learning Area `count_vowels`",
        "nll_prompt": -8.221037864685059
    },
    {
        "epoch": 74,
        "loss": 0.9135481119155884,
        "best_loss": 0.7615708112716675,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 14.69123649597168,
        "cur_std": 0.6686765193939209,
        "prompt": " Explan Welcome meets Alice impression% learning Area `count_vowels`",
        "nll_prompt": -8.222399711608887
    },
    {
        "epoch": 75,
        "loss": 0.9322347044944763,
        "best_loss": 0.7615708112716675,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 14.803437232971191,
        "cur_std": 0.6846728801727295,
        "prompt": "ists Welcome meets Alice impression% learning Area `count_vowels`",
        "nll_prompt": -8.778197288513184
    },
    {
        "epoch": 76,
        "loss": 0.8507356643676758,
        "best_loss": 0.7615708112716675,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 14.7943696975708,
        "cur_std": 0.6660758972167968,
        "prompt": "v Welcome meets Alice impression% learning Area `count_vowels`",
        "nll_prompt": -8.251603126525879
    },
    {
        "epoch": 77,
        "loss": 0.9105655550956726,
        "best_loss": 0.7615708112716675,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 15.755373001098633,
        "cur_std": 0.664278507232666,
        "prompt": "v Welcome meets Alice using% learning Area `count_vowels`",
        "nll_prompt": -8.21483039855957
    },
    {
        "epoch": 78,
        "loss": 1.0004377365112305,
        "best_loss": 0.7615708112716675,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 14.99218463897705,
        "cur_std": 0.6570285797119141,
        "prompt": "v Welcome meets Alice Compared% learning Area `count_vowels`",
        "nll_prompt": -8.705799102783203
    },
    {
        "epoch": 79,
        "loss": 1.0003093481063843,
        "best_loss": 0.7615708112716675,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 14.503183364868164,
        "cur_std": 0.6880806922912598,
        "prompt": "ore Welcome meets Alice Compared% learning Area `count_vowels`",
        "nll_prompt": -8.424151420593262
    },
    {
        "epoch": 80,
        "loss": 0.9858342409133911,
        "best_loss": 0.7615708112716675,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 15.640182495117188,
        "cur_std": 0.6458647727966309,
        "prompt": "ore Welcome Valent Alice Compared% learning Area `count_vowels`",
        "nll_prompt": -9.101386070251465
    },
    {
        "epoch": 81,
        "loss": 0.8585169315338135,
        "best_loss": 0.7615708112716675,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 14.54557991027832,
        "cur_std": 0.6621823787689209,
        "prompt": "ore Welcome Valent Alice Compared% Diplom Area `count_vowels`",
        "nll_prompt": -9.535895347595215
    },
    {
        "epoch": 82,
        "loss": 0.9476959705352783,
        "best_loss": 0.7615708112716675,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 13.67593002319336,
        "cur_std": 0.6533477783203125,
        "prompt": "ore Welcome Valent ver Compared% Diplom Area `count_vowels`",
        "nll_prompt": -9.687997817993164
    },
    {
        "epoch": 83,
        "loss": 0.8750254511833191,
        "best_loss": 0.7615708112716675,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 13.501787185668945,
        "cur_std": 0.6478981494903564,
        "prompt": "ore Welcome Valent Lanc Compared% Diplom Area `count_vowels`",
        "nll_prompt": -9.885828971862793
    },
    {
        "epoch": 84,
        "loss": 0.7957704663276672,
        "best_loss": 0.7615708112716675,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 12.943013191223145,
        "cur_std": 0.636937141418457,
        "prompt": "bys Welcome Valent Lanc Compared% Diplom Area `count_vowels`",
        "nll_prompt": -9.81692123413086
    },
    {
        "epoch": 85,
        "loss": 0.8662106990814209,
        "best_loss": 0.7615708112716675,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 12.779397010803223,
        "cur_std": 0.639444637298584,
        "prompt": "bys Welcome Valent Lanc Swift% Diplom Area `count_vowels`",
        "nll_prompt": -9.415196418762207
    },
    {
        "epoch": 86,
        "loss": 0.813125729560852,
        "best_loss": 0.7615708112716675,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 12.402671813964844,
        "cur_std": 0.6614578247070313,
        "prompt": "bys Levy Valent Lanc Swift% Diplom Area `count_vowels`",
        "nll_prompt": -9.368407249450684
    },
    {
        "epoch": 87,
        "loss": 0.8586053848266602,
        "best_loss": 0.7615708112716675,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 11.776862144470215,
        "cur_std": 0.6801093578338623,
        "prompt": "bys Levy Valent� Swift% Diplom Area `count_vowels`",
        "nll_prompt": -9.547924041748047
    },
    {
        "epoch": 88,
        "loss": 0.8140539526939392,
        "best_loss": 0.7615708112716675,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 11.51745319366455,
        "cur_std": 0.6365859031677246,
        "prompt": " z Levy Valent� Swift% Diplom Area `count_vowels`",
        "nll_prompt": -9.218578338623047
    },
    {
        "epoch": 89,
        "loss": 0.9022324681282043,
        "best_loss": 0.7615708112716675,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 12.719980239868164,
        "cur_std": 0.6717022895812989,
        "prompt": " Tal Levy Valent� Swift% Diplom Area `count_vowels`",
        "nll_prompt": -9.282135009765625
    },
    {
        "epoch": 90,
        "loss": 0.8057705760002136,
        "best_loss": 0.7615708112716675,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 12.198392868041992,
        "cur_std": 0.6345501899719238,
        "prompt": "65 Levy Valent� Swift% Diplom Area `count_vowels`",
        "nll_prompt": -9.261297225952148
    },
    {
        "epoch": 91,
        "loss": 0.8209515810012817,
        "best_loss": 0.7615708112716675,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 13.916565895080566,
        "cur_std": 0.627850341796875,
        "prompt": "65 Levy Valent� appropriate% Diplom Area `count_vowels`",
        "nll_prompt": -9.489874839782715
    },
    {
        "epoch": 92,
        "loss": 0.9253633618354797,
        "best_loss": 0.7615708112716675,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 13.947580337524414,
        "cur_std": 0.6320968627929687,
        "prompt": "65 Akin Valent� appropriate% Diplom Area `count_vowels`",
        "nll_prompt": -9.689355850219727
    },
    {
        "epoch": 93,
        "loss": 0.9315549731254578,
        "best_loss": 0.7615708112716675,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 12.447036743164062,
        "cur_std": 0.5870589733123779,
        "prompt": "65 Akin Valent� Steps% Diplom Area `count_vowels`",
        "nll_prompt": -9.71319580078125
    },
    {
        "epoch": 94,
        "loss": 0.9333723783493042,
        "best_loss": 0.7615708112716675,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 12.104947090148926,
        "cur_std": 0.5569448471069336,
        "prompt": "65 spine Valent� Steps% Diplom Area `count_vowels`",
        "nll_prompt": -9.62381362915039
    },
    {
        "epoch": 95,
        "loss": 0.8019135594367981,
        "best_loss": 0.7615708112716675,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 12.058368682861328,
        "cur_std": 0.5635054588317872,
        "prompt": "65 Corporate Valent� Steps% Diplom Area `count_vowels`",
        "nll_prompt": -9.482787132263184
    },
    {
        "epoch": 96,
        "loss": 0.8935561776161194,
        "best_loss": 0.7615708112716675,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 11.673749923706055,
        "cur_std": 0.5827084064483643,
        "prompt": "65 Corporate air� Steps% Diplom Area `count_vowels`",
        "nll_prompt": -8.634380340576172
    },
    {
        "epoch": 97,
        "loss": 0.9122853875160217,
        "best_loss": 0.7615708112716675,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 12.836137771606445,
        "cur_std": 0.5662726879119873,
        "prompt": "65 Corporate air� maiden% Diplom Area `count_vowels`",
        "nll_prompt": -8.546442031860352
    },
    {
        "epoch": 98,
        "loss": 0.8120238184928894,
        "best_loss": 0.7615708112716675,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 11.445671081542969,
        "cur_std": 0.5515254497528076,
        "prompt": "65 Corporate air� maiden% feedback Area `count_vowels`",
        "nll_prompt": -8.180922508239746
    },
    {
        "epoch": 99,
        "loss": 0.8430737853050232,
        "best_loss": 0.7615708112716675,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 11.760080337524414,
        "cur_std": 0.5551781177520752,
        "prompt": "65 Corporate air� cold% feedback Area `count_vowels`",
        "nll_prompt": -8.185650825500488
    },
    {
        "epoch": 100,
        "loss": 0.8980196714401245,
        "best_loss": 0.7615708112716675,
        "best_kl": 10.736991882324219,
        "best_std": 0.5756730556488037,
        "cur_kl": 13.46597671508789,
        "cur_std": 0.5748194694519043,
        "prompt": "65 Corporate air� cold% feedback Area `count_vowels�",
        "nll_prompt": -9.319023132324219
    }
]